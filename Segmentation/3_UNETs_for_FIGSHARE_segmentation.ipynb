{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MilanBandara/FYP/blob/main/3_UNETs_for_BRATS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0J3wcS2ZQoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.utils import normalize\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_image = (cv2.imread(\"F:\\FYP\\FYP\\FigshareDataset\\masks\\8.png\",cv2.IMREAD_GRAYSCALE)/255).astype(float)\n",
        "test_image = cv2.resize(test_image,(256,256))\n",
        "print(np.unique(test_image))\n",
        "plt.imshow(test_image, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GaSfIG6WmeI"
      },
      "source": [
        "# Creating the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkfksE-dXGP9"
      },
      "outputs": [],
      "source": [
        "# def load_img(img_dir, img_list):\n",
        "#     images=[]\n",
        "#     for i, image_name in enumerate(img_list):\n",
        "#         if (image_name.split('.')[1] == 'npy'):\n",
        "#             image = np.load(img_dir+image_name)\n",
        "#             # print(\"Image - \",image.shape)\n",
        "#             images.append(image)\n",
        "#     images = np.array(images)\n",
        "\n",
        "#     return(images)\n",
        "\n",
        "\n",
        "\n",
        "def load_img(img_dir, img_list):\n",
        "    images=[]\n",
        "    for i, image_name in enumerate(img_list):\n",
        "        if (image_name.split('.')[1] == 'png'):\n",
        "            image = cv2.imread(img_dir+\"/\"+image_name,0)\n",
        "            image = cv2.resize(image,(256,256))\n",
        "            images.append(image)\n",
        "    images = np.array(images)\n",
        "\n",
        "    return(images)\n",
        "\n",
        "\n",
        "def load_msk(img_dir, img_list):\n",
        "    images=[]\n",
        "    for i, image_name in enumerate(img_list):\n",
        "        if (image_name.split('.')[1] == 'png'):\n",
        "            image = (cv2.imread(img_dir+\"/\"+image_name,0)/255).astype(float)\n",
        "            # print(np.unique(image))\n",
        "            image = cv2.resize(image,(256,256))\n",
        "            # print(\"before - \",image.shape)\n",
        "            image = to_categorical(image, num_classes=2)\n",
        "            # print(\"After - \",image.shape)\n",
        "            images.append(image)\n",
        "    images = np.array(images)\n",
        "\n",
        "    return(images)\n",
        "\n",
        "def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size):\n",
        "\n",
        "    L = len(img_list)\n",
        "\n",
        "    #keras needs the generator infinite, so we will use while true\n",
        "    while True:\n",
        "\n",
        "        batch_start = 0\n",
        "        batch_end = batch_size\n",
        "\n",
        "        while batch_start < L:\n",
        "            limit = min(batch_end, L)\n",
        "\n",
        "            X = load_img(img_dir, img_list[batch_start:limit])\n",
        "            Y = load_msk(mask_dir, mask_list[batch_start:limit])\n",
        "\n",
        "            yield (X,Y) #a tuple with two numpy arrays with batch_size samples\n",
        "            batch_start += batch_size\n",
        "            batch_end += batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo5ycNUKXM1g"
      },
      "source": [
        "# Making the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9u3OW40YIjy"
      },
      "outputs": [],
      "source": [
        "train_img_dir = r\"F:\\FYP\\FYP\\FigshareDataset\\images\"\n",
        "train_mask_dir = r\"F:\\FYP\\FYP\\FigshareDataset\\masks\"\n",
        "train_img_list=os.listdir(train_img_dir)\n",
        "train_mask_list = os.listdir(train_mask_dir)\n",
        "\n",
        "# val_img_dir = \"/content/drive/MyDrive/BRATS_train_data/X_validation/\"\n",
        "# val_mask_dir = \"/content/drive/MyDrive/BRATS_train_data/Y_validation/\"\n",
        "# val_img_list=os.listdir(val_img_dir)\n",
        "# val_mask_list = os.listdir(val_mask_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqqnGK6PXMCe"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_img_datagen = imageLoader(train_img_dir, train_img_list,\n",
        "                                train_mask_dir, train_mask_list, batch_size)\n",
        "\n",
        "# val_img_datagen = imageLoader(val_img_dir, val_img_list,\n",
        "#                                 val_mask_dir, val_mask_list, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO3f9BZXXfGf"
      },
      "source": [
        "# Verifying the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdPUPs1fXc4r"
      },
      "outputs": [],
      "source": [
        "#Verify generator.... In python 3 next() is renamed as __next__()\n",
        "img, msk = train_img_datagen.__next__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoY-LQEQZB3U"
      },
      "source": [
        "# Visualizing the loaded images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "dODfFiTVbJ7j",
        "outputId": "b759fdd0-2342-4dab-8b11-745e0b83af94"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "data_point = random.randint(0, batch_size)\n",
        "# data_point = 3\n",
        "\n",
        "original_labels = np.expand_dims(msk[data_point,:,:,:], axis=0)\n",
        "original_labels = np.argmax(original_labels, axis=3)\n",
        "print(original_labels.shape)\n",
        "\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(img[data_point,:], cmap='gray')\n",
        "plt.title('Scan')\n",
        "plt.subplot(122)\n",
        "plt.imshow(original_labels[0,:,:], cmap='gray')\n",
        "plt.title('Complete label')\n",
        "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0tjF0qzhdsX"
      },
      "source": [
        "# Defining 3 UNET architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQwDicNzhick"
      },
      "outputs": [],
      "source": [
        "# https://youtu.be/L5iV5BHkMzM\n",
        "\"\"\"\n",
        "\n",
        "Attention U-net:\n",
        "https://arxiv.org/pdf/1804.03999.pdf\n",
        "\n",
        "Recurrent residual Unet (R2U-Net) paper\n",
        "https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n",
        "(Check fig 4.)\n",
        "\n",
        "Note: Batch normalization should be performed over channels after a convolution,\n",
        "In the following code axis is set to 3 as our inputs are of shape\n",
        "[None, height, width, channel]. Channel is axis=3.\n",
        "\n",
        "Original code from below link but heavily modified.\n",
        "https://github.com/MoleImg/Attention_UNet/blob/master/AttResUNet.py\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "A few useful metrics and losses\n",
        "'''\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "##############################################################\n",
        "'''\n",
        "Useful blocks to build Unet\n",
        "\n",
        "conv - BN - Activation - conv - BN - Activation - Dropout (if enabled)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
        "\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\",kernel_initializer='he_normal',activation = \"relu\")(x)\n",
        "    conv = layers.Dropout(dropout)(conv)\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\",kernel_initializer='he_normal',activation = \"relu\")(conv)\n",
        "    return conv\n",
        "\n",
        "\n",
        "def repeat_elem(tensor, rep):\n",
        "    # lambda function to repeat Repeats the elements of a tensor along an axis\n",
        "    #by a factor of rep.\n",
        "    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape\n",
        "    #(None, 256,256,6), if specified axis=3 and rep=2.\n",
        "\n",
        "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
        "                          arguments={'repnum': rep})(tensor)\n",
        "\n",
        "\n",
        "def res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
        "    '''\n",
        "    Residual convolutional layer.\n",
        "    Two variants....\n",
        "    Either put activation function before the addition with shortcut\n",
        "    or after the addition (which would be as proposed in the original resNet).\n",
        "\n",
        "    1. conv - BN - Activation - conv - BN - Activation\n",
        "                                          - shortcut  - BN - shortcut+BN\n",
        "\n",
        "    2. conv - BN - Activation - conv - BN\n",
        "                                     - shortcut  - BN - shortcut+BN - Activation\n",
        "\n",
        "    Check fig 4 in https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n",
        "    '''\n",
        "\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)\n",
        "    if batch_norm is True:\n",
        "        conv = layers.BatchNormalization(axis=3)(conv)\n",
        "    conv = layers.Activation('relu')(conv)\n",
        "\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n",
        "    if batch_norm is True:\n",
        "        conv = layers.BatchNormalization(axis=3)(conv)\n",
        "    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n",
        "    if dropout > 0:\n",
        "        conv = layers.Dropout(dropout)(conv)\n",
        "\n",
        "    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n",
        "    if batch_norm is True:\n",
        "        shortcut = layers.BatchNormalization(axis=3)(shortcut)\n",
        "\n",
        "    res_path = layers.add([shortcut, conv])\n",
        "    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)\n",
        "    return res_path\n",
        "\n",
        "def gating_signal(input, out_size, batch_norm=False):\n",
        "    \"\"\"\n",
        "    resize the down layer feature map into the same dimension as the up layer feature map\n",
        "    using 1x1 conv\n",
        "    :return: the gating feature map with the same dimension of the up layer feature map\n",
        "    \"\"\"\n",
        "    x = layers.Conv2D(out_size, (1, 1), padding='same',kernel_initializer='he_normal')(input)\n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def attention_block(x, gating, inter_shape):\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gating)\n",
        "\n",
        "# Getting the x signal to the same shape as the gating signal\n",
        "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same',kernel_initializer='he_normal')(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "# Getting the gating signal to the same number of filters as the inter_shape\n",
        "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same',kernel_initializer='he_normal')(gating)\n",
        "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
        "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
        "                                 padding='same')(phi_g)  # 16\n",
        "\n",
        "    concat_xg = layers.add([upsample_g, theta_x])\n",
        "    act_xg = layers.Activation('relu')(concat_xg)\n",
        "    psi = layers.Conv2D(1, (1, 1), padding='same',kernel_initializer='he_normal')(act_xg)\n",
        "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
        "\n",
        "    y = layers.multiply([upsample_psi, x])\n",
        "\n",
        "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same',kernel_initializer='he_normal')(y)\n",
        "    result_bn = layers.BatchNormalization()(result)\n",
        "    return result_bn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def UNet(input_shape, NUM_CLASSES=2, dropout_rate=0.4, batch_norm=True):\n",
        "    '''\n",
        "    UNet,\n",
        "\n",
        "    '''\n",
        "    # network structure\n",
        "    FILTER_NUM = 16 # number of filters for the first layer\n",
        "    FILTER_SIZE = 3 # size of the convolutional filter\n",
        "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
        "\n",
        "\n",
        "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
        "\n",
        "    # Downsampling layers\n",
        "    # DownRes 1, convolution + pooling\n",
        "    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, 0.3, batch_norm)#Conv2D > dropout > Conv2D\n",
        "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
        "    # DownRes 2\n",
        "    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, 0.3, batch_norm)\n",
        "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
        "    # DownRes 3\n",
        "    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, 0.4, batch_norm)\n",
        "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
        "    # DownRes 4\n",
        "    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, 0.4, batch_norm)\n",
        "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
        "    # DownRes 5, convolution only\n",
        "    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, 0.5, batch_norm)\n",
        "\n",
        "    # Upsampling layers\n",
        "\n",
        "    #up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
        "    up_16 = layers.Conv2DTranspose(8*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(conv_8)\n",
        "    up_16 = layers.concatenate([up_16, conv_16], axis=3)\n",
        "    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, 0.4, batch_norm)\n",
        "    # UpRes 7\n",
        "\n",
        "    up_32 = layers.Conv2DTranspose(4*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_16)\n",
        "    up_32 = layers.concatenate([up_32, conv_32], axis=3)\n",
        "    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, 0.4, batch_norm)\n",
        "    # UpRes 8\n",
        "\n",
        "    up_64 = layers.Conv2DTranspose(2*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_32)\n",
        "    up_64 = layers.concatenate([up_64, conv_64], axis=3)\n",
        "    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, 0.3, batch_norm)\n",
        "    # UpRes 9\n",
        "\n",
        "    up_128 = layers.Conv2DTranspose(FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_64)\n",
        "    up_128 = layers.concatenate([up_128, conv_128], axis=3)\n",
        "    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, 0.3, batch_norm)\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "\n",
        "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1),kernel_initializer='he_normal',activation = 'sigmoid')(up_conv_128)\n",
        "    #conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    #conv_final = layers.Activation('softmax')(conv_final)  #Change to softmax for multichannel\n",
        "\n",
        "    # Model\n",
        "    model = models.Model(inputs, conv_final, name=\"UNet\")\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def Attention_UNet(input_shape, NUM_CLASSES=4, dropout_rate=0.4, batch_norm=False):\n",
        "    '''\n",
        "    Attention UNet,\n",
        "\n",
        "    '''\n",
        "    # network structure\n",
        "    FILTER_NUM = 16 # number of basic filters for the first layer\n",
        "    FILTER_SIZE = 3 # size of the convolutional filter\n",
        "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
        "\n",
        "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
        "\n",
        "    # Downsampling layers\n",
        "    # DownRes 1, convolution + pooling\n",
        "    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, 0.3, batch_norm)#Conv2D > dropout > Conv2D\n",
        "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
        "    # DownRes 2\n",
        "    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, 0.3, batch_norm)\n",
        "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
        "    # DownRes 3\n",
        "    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, 0.4, batch_norm)\n",
        "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
        "    # DownRes 4\n",
        "    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, 0.4, batch_norm)\n",
        "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
        "    # DownRes 5, convolution only\n",
        "    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, 0.5, batch_norm)\n",
        "\n",
        "    # Upsampling layers\n",
        "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
        "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
        "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
        "    up_16 = layers.Conv2DTranspose(8*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(conv_8)\n",
        "    up_16 = layers.concatenate([up_16, att_16], axis=3)\n",
        "    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, 0.4, batch_norm)\n",
        "    # UpRes 7\n",
        "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
        "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
        "    up_32 = layers.Conv2DTranspose(4*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_16)\n",
        "    up_32 = layers.concatenate([up_32, att_32], axis=3)\n",
        "    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, 0.4, batch_norm)\n",
        "    # UpRes 8\n",
        "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
        "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
        "    up_64 = layers.Conv2DTranspose(2*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_32)\n",
        "    up_64 = layers.concatenate([up_64, att_64], axis=3)\n",
        "    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, 0.3, batch_norm)\n",
        "    # UpRes 9\n",
        "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
        "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
        "    up_128 = layers.Conv2DTranspose(FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_64)\n",
        "    up_128 = layers.concatenate([up_128, att_128], axis=3)\n",
        "    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, 0.3, batch_norm)\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1),kernel_initializer='he_normal',activation = 'softmax')(up_conv_128)\n",
        "    #conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    #conv_final = layers.Activation('softmax')(conv_final)  #Change to softmax for multichannel\n",
        "\n",
        "    # Model integration\n",
        "    model = models.Model(inputs, conv_final, name=\"Attention_UNet\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def Attention_ResUNet(input_shape, NUM_CLASSES=4, dropout_rate=0.4, batch_norm=True):\n",
        "    '''\n",
        "    Rsidual UNet, with attention\n",
        "\n",
        "    '''\n",
        "    # network structure\n",
        "    FILTER_NUM = 32 # number of basic filters for the first layer\n",
        "    FILTER_SIZE = 3 # size of the convolutional filter\n",
        "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
        "    # input data\n",
        "    # dimension of the image depth\n",
        "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
        "    axis = 3\n",
        "\n",
        "    # Downsampling layers\n",
        "    # DownRes 1, double residual convolution + pooling\n",
        "    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
        "    # DownRes 2\n",
        "    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
        "    # DownRes 3\n",
        "    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
        "    # DownRes 4\n",
        "    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
        "    # DownRes 5, convolution only\n",
        "    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # Upsampling layers\n",
        "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
        "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
        "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
        "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
        "    up_16 = layers.concatenate([up_16, att_16], axis=axis)\n",
        "    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 7\n",
        "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
        "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
        "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
        "    up_32 = layers.concatenate([up_32, att_32], axis=axis)\n",
        "    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 8\n",
        "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
        "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
        "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
        "    up_64 = layers.concatenate([up_64, att_64], axis=axis)\n",
        "    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 9\n",
        "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
        "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
        "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
        "    up_128 = layers.concatenate([up_128, att_128], axis=axis)\n",
        "    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "\n",
        "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n",
        "    conv_final = layers.BatchNormalization(axis=axis)(conv_final)\n",
        "    conv_final = layers.Activation('softmax')(conv_final)  #Change to softmax for multichannel\n",
        "\n",
        "    # Model integration\n",
        "    model = models.Model(inputs, conv_final, name=\"AttentionResUNet\")\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en-rIizgiZZY"
      },
      "outputs": [],
      "source": [
        "# input_shape = (240,240,4)\n",
        "# # UNet(input_shape, NUM_CLASSES=4, dropout_rate=0.0, batch_norm=True)\n",
        "# # Attention_UNet(input_shape, NUM_CLASSES=4, dropout_rate=0.0, batch_norm=True)\n",
        "# Attention_ResUNet(input_shape, NUM_CLASSES=4, dropout_rate=0.0, batch_norm=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH-O4iCbjhcU"
      },
      "outputs": [],
      "source": [
        "###############################################################\n",
        "# from sklearn.utils import class_weight\n",
        "# class_weights = class_weight.compute_class_weight('balanced',\n",
        "#                                                  np.unique(train_masks_reshaped_encoded),\n",
        "#                                                  train_masks_reshaped_encoded)\n",
        "# print(\"Class weights are...:\", class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7zpyoBZT3CI"
      },
      "source": [
        "# Defining loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNG05jfhT2BZ"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)  # -1 ultiplied as we want to minimize this value as loss function\n",
        "\n",
        "# # Typical tf.keras API usage\n",
        "# import tensorflow as tf\n",
        "# from focal_loss import BinaryFocalLoss,SparseCategoricalFocalLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94qTpaQoUDgS"
      },
      "outputs": [],
      "source": [
        "# import segmentation_models_3D as sm\n",
        "# dice_loss = sm.losses.DiceLoss(class_weights=np.array([wt0, wt1, wt2, wt3]))\n",
        "# focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "# total_loss = dice_loss + (1 * focal_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4Y1Ji2hVWl5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self, alpha=0.25, gamma=4.0):\n",
        "    super(FocalLoss, self).__init__()\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def call(self, y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Inputs and targets should be of shape (batch_size, num_classes, height, width)\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the cross-entropy loss\n",
        "    ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Calculate the scaling factor\n",
        "    pt = tf.math.exp(-ce_loss)\n",
        "    #scaling_factor = self.alpha * (1 - pt) ** self.gamma\n",
        "    scaling_factor = (1 - pt) ** self.gamma\n",
        "\n",
        "    # Calculate the Focal loss\n",
        "    focal_loss = scaling_factor * ce_loss\n",
        "\n",
        "    # Return the average Focal loss over the batch\n",
        "    return tf.reduce_mean(focal_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqpXKkoHNeNY"
      },
      "source": [
        "# Creating a callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEA_6NadLL2A"
      },
      "outputs": [],
      "source": [
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"/content/drive/MyDrive/models/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Calculate the number of batches per epoch\n",
        "import math\n",
        "n_batches = 3000 / batch_size\n",
        "n_batches = math.ceil(n_batches)    # round up the number of batches to the nearest whole integer\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=10*n_batches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Drrib_NjEU0",
        "outputId": "499a3292-259c-41a0-a4c6-1e174d83e970"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH  =  256\n",
        "IMG_CHANNELS = 1\n",
        "\n",
        "\n",
        "input_shape = (IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS)\n",
        "\n",
        "\n",
        "model = UNet(input_shape, NUM_CLASSES=2, dropout_rate=0.4, batch_norm=False)\n",
        "# model.compile(optimizer='adam', loss= tf.keras.losses.CategoricalFocalCrossentropy(alpha=1,gamma=4), metrics=[jacard_coef])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtMbEkG3qVa0"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = len(train_img_list)//batch_size\n",
        "# val_steps_per_epoch = len(val_img_list)//batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "YywvHDtpjG9z",
        "outputId": "4425699d-9bf8-4e93-ec00-cfe6631d6618"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_img_datagen,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    verbose=1,\n",
        "                    epochs=103,\n",
        "                    # validation_data=val_img_datagen,\n",
        "                    # validation_steps=val_steps_per_epoch,\n",
        "                    shuffle=False,\n",
        "                    # callbacks=[cp_callback],\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuoWi4lTkPc6"
      },
      "outputs": [],
      "source": [
        "###\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGELS4ok1g2x"
      },
      "outputs": [],
      "source": [
        "for i in history.history.items():\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOOR87_9kYxy"
      },
      "outputs": [],
      "source": [
        "acc = history.history['jacard_coef']\n",
        "val_acc = history.history['val_jacard_coef']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcBdFfESDgF3"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/drive/MyDrive/models/training_loss.npy\",np.array(loss))\n",
        "np.save(\"/content/drive/MyDrive/models/val_loss.npy\",np.array(val_loss))\n",
        "np.save(\"/content/drive/MyDrive/models/training_jaccard.npy\",np.array(acc))\n",
        "np.save(\"/content/drive/MyDrive/models/val_jaccard.npy\",np.array(val_acc))\n",
        "np.save(\"/content/drive/MyDrive/models/epochs.npy\",range(1, len(np.array(loss)) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJDNrwwN1J_"
      },
      "outputs": [],
      "source": [
        "os.listdir(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYkv2D_5N47t"
      },
      "outputs": [],
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUblHKOOOD2e"
      },
      "outputs": [],
      "source": [
        "# Create a new model instance\n",
        "\n",
        "model = Attention_UNet(input_shape, NUM_CLASSES=4, dropout_rate=0.2, batch_norm=True)\n",
        "model.compile(optimizer='adam', loss= FocalLoss(), metrics=[jacard_coef])\n",
        "\n",
        "# Load the previously saved weights\n",
        "model.load_weights(\"/content/drive/MyDrive/models/cp-0041.ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFOGEiJDGXdx"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_img_datagen,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    verbose=1,\n",
        "                    epochs=100,\n",
        "                    validation_data=val_img_datagen,\n",
        "                    validation_steps=val_steps_per_epoch,\n",
        "                    shuffle=False,\n",
        "                    callbacks=[cp_callback],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6w7tqWZke9a"
      },
      "outputs": [],
      "source": [
        "#Predict on a few images\n",
        "#model = get_model()\n",
        "#model.load_weights('???.hdf5')\n",
        "img, msk = val_img_datagen.__next__()\n",
        "import random\n",
        "test_img_number = random.randint(0, len(img))#6 is good\n",
        "test_img = img[test_img_number]\n",
        "ground_truth=msk[test_img_number]\n",
        "# test_img_norm=test_img[:,:,0][:,:,None]\n",
        "test_img_input=np.expand_dims(test_img, 0)\n",
        "prediction = (model.predict(test_img_input))\n",
        "predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
        "print(prediction.shape)\n",
        "print(ground_truth.shape)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img[:,:,0], cmap='gray') #only the first channel is displayed\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.title('Original Label')\n",
        "displayed_label = np.argmax(ground_truth, axis=2)\n",
        "plt.imshow(displayed_label, cmap='jet')\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.title('Predicted Label')\n",
        "plt.imshow(predicted_img, cmap='jet')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsYwxbvVjlJp"
      },
      "outputs": [],
      "source": [
        "print(test_img_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK21FSbuoPNx"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/models/my_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qth9Izdk385O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

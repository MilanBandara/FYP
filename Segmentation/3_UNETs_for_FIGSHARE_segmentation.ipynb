{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MilanBandara/FYP/blob/main/3_UNETs_for_BRATS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y0J3wcS2ZQoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.utils import normalize\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([0., 1.]), array([63430,  2106], dtype=int64))\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGiCAYAAAC/NyLhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAln0lEQVR4nO3de3BUZZ7/8U93Li0ROjGEpJMhQUDkYpBZuWRad/BCloShKBnYGkV2hrEoKDBMjYKME2sFsaY2uzo1uzW7rO7U7oLOzjLKrGBBibUMlyASUBhRAc0SjBOQdAKJ6SYXcuvn98cs/duWALkfnuT9qnqq0uc85/T3PHbz8ZzzdLfLGGMEAIAl3E4XAABAVxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqzgWXBs3btTtt9+uW265RTk5OXr//fedKgUAYBFHguv111/X6tWrtX79ev3hD3/QlClTlJeXp+rqaifKAQBYxOXEl+zm5ORo+vTp+qd/+idJUjgcVmZmpn70ox/ppz/9aX+XAwCwSGx/P2FLS4uOHTumwsLCyDK3263c3FyVlJR0uE1zc7Oam5sjj8PhsGprazV8+HC5XK4+rxkA0LuMMbp06ZIyMjLkdnft4l+/B9fFixfV3t6utLS0qOVpaWn67LPPOtymqKhIGzZs6I/yAAD96OzZsxo5cmSXtrFiVmFhYaGCwWCkVVRUOF0SAKAXDBs2rMvb9PsZV0pKimJiYlRVVRW1vKqqSj6fr8NtPB6PPB5Pf5QHAOhH3bnd0+9nXPHx8Zo6dar27NkTWRYOh7Vnzx75/f7+LgcAYJl+P+OSpNWrV2vJkiWaNm2aZsyYoX/4h39QQ0ODHn/8cSfKAQBYxJHgeuSRR3ThwgWtW7dOgUBA3/zmN/XOO+9cNWEDAICvc+RzXD0VCoWUmJjodBkAgB4KBoPyer1d2saKWYUAAFxBcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKzS68H1/PPPy+VyRbUJEyZE1l++fFkFBQUaPny4hg4dqoULF6qqqqq3ywAADFB9csZ11113qbKyMtIOHjwYWffUU09px44d2rp1q4qLi3X+/HktWLCgL8oAAAxAsX2y09hY+Xy+q5YHg0H927/9m/7zP/9TDz30kCRp06ZNmjhxog4fPqxvfetbfVEOAGAA6ZMzrtOnTysjI0NjxozR4sWLVVFRIUk6duyYWltblZubG+k7YcIEZWVlqaSkpC9KAQAMML1+xpWTk6PNmzdr/Pjxqqys1IYNG/Ttb39bJ06cUCAQUHx8vJKSkqK2SUtLUyAQuOY+m5ub1dzcHHkcCoV6u2wAgCV6PbjmzJkT+fvuu+9WTk6ORo0apTfeeENDhgzp1j6Lioq0YcOG3ioRAGCxPp8On5SUpDvvvFNlZWXy+XxqaWlRXV1dVJ+qqqoO74ldUVhYqGAwGGlnz57t46oBADerPg+u+vp6nTlzRunp6Zo6dari4uK0Z8+eyPrS0lJVVFTI7/dfcx8ej0derzeqAQAGp16/VPj0009r3rx5GjVqlM6fP6/169crJiZGixYtUmJiopYuXarVq1crOTlZXq9XP/rRj+T3+5lRCADolF4PrnPnzmnRokWqqanRiBEj9Od//uc6fPiwRowYIUn6+7//e7ndbi1cuFDNzc3Ky8vTP//zP/d2GQCAAcpljDFOF9FVoVBIiYmJTpcBAOihYDDY5ds/fFchAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqsU4XAMB+hYWFeuCBB6KWhcNhLV26VOfPn3emKAxYBBeAbhk7dqyysrIkSX/xF3+hBx98MGq9MUazZs3SuXPnFA6H9d5776mtrc2JUjHAuIwxxukiuioUCikxMdHpMoBB7aWXXtLTTz/dqb719fUaNWqUamtr+7gq2CYYDMrr9XZpG864AHTaokWL9Oyzz0qSfD5fp7dLSEjQwYMH1d7erurqauXl5XH2hW4juIBBLC4uTn/5l3+p+Ph4NTQ06L/+67/09YswmZmZeuihhyRJs2bNUnZ2dpefx+12a+LEiZKkuro6LVmyRG1tbaqtrdWOHTt6fiAYVLhUCAxC8fHxcrlcGjZsmMrKypSYmKizZ89q4sSJV50JzZs3T1u3bu2TOj766CPl5ORI+tNkjtbW1j55Hty8unOpkOACBhm3261Dhw5p1KhRcrvdGjFihFwul8LhsC5cuHDVGdctt9yipKSkPqmlra1NFy9elCT9/ve/1/e///0+eR7cvLjHBeC6xo8fr0ceeUR33nmnbrvttqh1brdbaWlp/VpPbGxs5F7ZjBkz9Nd//deSpGPHjmnXrl39WgvswRkXMMDFxsZGzpi+853v6NVXX3W2oE547bXXtGbNGklSQ0ODmpqaHK4IfYVLhQCuMm3aNB04cECSFBMTo/j4eIcrurH29na1tLRIktavX6+XXnrJ4YrQV7hUCCDKypUrlZ+fryFDhjhdSpfExMREav7e976n8ePHS5L+9V//VYcPH3ayNNwECC5gAIqNjdUdd9yh+fPna/bs2U6X0yPTpk3TtGnTJEn/8z//o7q6umv2NcaorKxM7e3t/VQdnMClQmAAysrKUllZmeLi4pwupddd75+shoYGjR49OjJTETc/LhUC0PLly/XYY48pNnZgvr1dLtc11yUkJOj1119Xa2uramtrtWTJEj4bNgANzFc2MAjFxMTI7/crNzdX999/v9PlOMLtdke+5aO2tlYPPvigPvroI1VVVTlcGXoTlwqBAcLr9aqiooL3xtcsWbJEr732mtNl4Bq6c6mQH5IEBoDFixfrvffe07Bhw5wu5abzs5/9rM++sgrOILiAAWDEiBHKzs6W281b+usyMzOVk5Ojv/qrv+r3bwZB3+BVDlguPj5+wE7E6C2ZmZn69a9/rUmTJjldCnoBr3bAYjExMXr33Xc1YcIEp0sB+g1nXIDlUlNTu3xzG7AZwQVYKj4+XikpKYqJiXG6FGskJSUR8gMAwQVY6jvf+Y4+//xzjRw50ulSrLFlyxb9x3/8h9NloIe4xwVYKiYmRgkJCU6XYRWPxyOPx+N0GeghzrgAAFYhuAAAViG4AEsdPHhQ8+bNU3V1tdOlAP2K4AIsVVVVpV27dqmxsdHpUoB+RXABAKzCrEIAg8by5cu1Z88ep8tAD3X5jOvAgQOaN2+eMjIy5HK5tH379qj1xhitW7dO6enpGjJkiHJzc3X69OmoPrW1tVq8eLG8Xq+SkpK0dOlS1dfX9+hAAOBGysrK9PnnnztdBnqoy8HV0NCgKVOmaOPGjR2uf/HFF/XLX/5Sr7zyio4cOaJbb71VeXl5unz5cqTP4sWLdfLkSe3evVs7d+7UgQMHtHz58u4fBQBg8DA9IMls27Yt8jgcDhufz2deeumlyLK6ujrj8XjMli1bjDHGnDp1ykgyH3zwQaTPrl27jMvlMl9++WWnnjcYDBpJNNqgbzExMaa8vLwnb+NB5cEHH3T8vxktugWDwS7/d+zVyRnl5eUKBALKzc2NLEtMTFROTo5KSkokSSUlJUpKStK0adMifXJzc+V2u3XkyJHeLAcAMAD16uSMQCAgSVf9WFtaWlpkXSAQUGpqanQRsbFKTk6O9Pm65uZmNTc3Rx6HQqHeLBuwUnx8vJKSkviS3U5ob29XMBhUa2ur06WgF1gxHb6oqEiJiYmRlpmZ6XRJgOPmzJmj8vJyvmS3Ez799FNlZWXp0KFDTpeCXtCrweXz+ST96YOR/1dVVVVknc/nu+qT/m1tbaqtrY30+brCwkIFg8FIO3v2bG+WDVgpNjZWCQkJcrlcTpdy0zPGqKGhQeFw2OlS0At6NbhGjx4tn88X9TmJUCikI0eOyO/3S5L8fr/q6up07NixSJ+9e/cqHA4rJyenw/16PB55vd6oBgxmmZmZSk9Pd7oMwBFdvsdVX1+vsrKyyOPy8nIdP35cycnJysrK0pNPPqmf/exnGjdunEaPHq3nnntOGRkZmj9/viRp4sSJys/P17Jly/TKK6+otbVVq1at0qOPPqqMjIxeOzBgIHv99df1rW99y+kyAGd0dRrivn37OpzSuGTJEmPMn6bEP/fccyYtLc14PB4za9YsU1paGrWPmpoas2jRIjN06FDj9XrN448/bi5dutTpGpgOTxusbfz48WbHjh3mq6++6upbd9D6xS9+YWbOnOn4fztax6070+Fdxhgjy4RCISUmJjpdBtDvcnJydPjwYafLsMoPfvAD/frXv3a6DFxDMBjs8u0fK2YVAgBwBcEFALAKwQVYpKamRlu2bFFNTY3Tpdz0Ll++rDfeeEPl5eVOl4Jexj0uwELvvvuu/H4/35pxHZWVlbr99tvV0tLidCm4Du5xAYPEggUL9MQTTzhdBuAIgguw0IULF3To0CH9/Oc/57s7MegQXIClTpw4oZ/+9KcqLy9XU1OT0+XcVC5fvqxgMOh0GegjBBdgsfb2dt17773asGGD06XcVH7+859r2rRp3N8aoAguwHKNjY1RP/uDP/0UUkNDg9NloI8QXAAAqxBcAACr9OovIAPoX263W7/61a+u+ZNAg01LS4uWLl3K9zkOcHwAGbBYTEyMysrKdPvttztdyk2hqalJWVlZunjxotOloJP4ADIAYMAjuABLPfjgg3r//ff5Adb/tX37dt17772qq6tzuhT0Me5xAZZKTk7WPffc43QZN40LFy7o+PHjTpeBfsAZF2CpcDistrY2p8u4KbS3t6u9vd3pMtBPmJwBWGrIkCFKS0vTu+++q5EjRzpdjmOMMXrooYf00Ucf6auvvnK6HHRRdyZncKkQsFRTU5POnj07qM+6ysvL9bvf/U4nT54ktAYRgguAlerr63X06FH95Cc/cboU9DOCC4CVvve972n//v1OlwEHMDkDsFg4HNbzzz+vN9980+lS+l1jYyM/5zJIccYFWMwYo1dffVVut1vf/OY3I8tjY2OVlZXlXGF9qKWlRefOndPly5edLgUOYVYhMAC4XC653f//AkpmZqbKysoUExPjYFV94+OPP9Y999zD9PcBglmFwCBljIn6h7yqqkoLFy7UM888I7/f72BlveOrr77S8uXL1dbWpmAwSGgNcgQXMAA1NTXprbfe0owZMzR06FBNnjzZ6ZK6pKqqSmfOnIk8vnjxorZv3z6op/7j/+NSITDAzZgxQ0eOHHG6jC75l3/5F61YscLpMtAPuFQI4ConT57U9OnT9e///u99eub12Wef6fvf/36v7OvChQu9sh8MTAQXMMA1NDTo6NGj2rFjhy5duqR777231/bd1NSkt99+W8YYff755zp69Giv7Ru4Fi4VAoPIww8/rN/97neKje3Z/7O2t7fLGKNz585p3Lhx3HtCt3GpEMB1/fd//7fGjRun4uLibn/Oq7GxUX6/X8FgUG1tbYQW+h3BBQwiTU1N+uMf/6hf/epXGj58uOLi4rRs2TJ5PJ7rbmeM0aZNmxQMBtXc3KzTp0/zrRVwDJcKgUFs6NChOnny5A3fT+FwWPfcc4+++OKL/ikMgwaXCgF0SX19ve666y65XK5O9QVuBgQXMMgRSLAN3w4PALAKwQUAsArBBQCwCsEFALAKwQUAsArBBQCwCsEFALAKwQUAsArBBQCwCsEFALAKwQUAsArBBQCwCsEFALAKwQUAsArBBQCwCsEFALAKwQUAsArBBQCwCsEFALAKwQUAsArBBQCwCsEFALAKwQUAsArBBQCwCsEFALAKwQUAsArBBQCwSpeD68CBA5o3b54yMjLkcrm0ffv2qPU//OEP5XK5olp+fn5Un9raWi1evFher1dJSUlaunSp6uvre3QgAIDBocvB1dDQoClTpmjjxo3X7JOfn6/KyspI27JlS9T6xYsX6+TJk9q9e7d27typAwcOaPny5V2vHgAw+JgekGS2bdsWtWzJkiXm4YcfvuY2p06dMpLMBx98EFm2a9cu43K5zJdfftmp5w0Gg0YSjUaj0SxvwWCwy9nTJ/e49u/fr9TUVI0fP14rV65UTU1NZF1JSYmSkpI0bdq0yLLc3Fy53W4dOXKkw/01NzcrFApFNQDA4NTrwZWfn6/XXntNe/bs0d/93d+puLhYc+bMUXt7uyQpEAgoNTU1apvY2FglJycrEAh0uM+ioiIlJiZGWmZmZm+XDQCwRGxv7/DRRx+N/D158mTdfffdGjt2rPbv369Zs2Z1a5+FhYVavXp15HEoFCK8AGCQ6vPp8GPGjFFKSorKysokST6fT9XV1VF92traVFtbK5/P1+E+PB6PvF5vVAMADE59Hlznzp1TTU2N0tPTJUl+v191dXU6duxYpM/evXsVDoeVk5PT1+UAACzX5UuF9fX1kbMnSSovL9fx48eVnJys5ORkbdiwQQsXLpTP59OZM2f0k5/8RHfccYfy8vIkSRMnTlR+fr6WLVumV155Ra2trVq1apUeffRRZWRk9N6RAQAGpq5OQ9y3b1+HUxqXLFliGhsbzezZs82IESNMXFycGTVqlFm2bJkJBAJR+6ipqTGLFi0yQ4cONV6v1zz++OPm0qVLna6B6fA0Go02MFp3psO7jDFGlgmFQkpMTHS6DABADwWDwS7PW+C7CgEAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAVulScBUVFWn69OkaNmyYUlNTNX/+fJWWlkb1uXz5sgoKCjR8+HANHTpUCxcuVFVVVVSfiooKzZ07VwkJCUpNTdXatWvV1tbW86MBAAx4XQqu4uJiFRQU6PDhw9q9e7daW1s1e/ZsNTQ0RPo89dRT2rFjh7Zu3ari4mKdP39eCxYsiKxvb2/X3Llz1dLSokOHDunVV1/V5s2btW7dut47KgDAwGV6oLq62kgyxcXFxhhj6urqTFxcnNm6dWukz6effmokmZKSEmOMMW+//bZxu90mEAhE+rz88svG6/Wa5ubmTj1vMBg0kmg0Go1meQsGg13Onh7d4woGg5Kk5ORkSdKxY8fU2tqq3NzcSJ8JEyYoKytLJSUlkqSSkhJNnjxZaWlpkT55eXkKhUI6efJkh8/T3NysUCgU1QAAg1O3gyscDuvJJ5/Ufffdp+zsbElSIBBQfHy8kpKSovqmpaUpEAhE+vzf0Lqy/sq6jhQVFSkxMTHSMjMzu1s2AMBy3Q6ugoICnThxQr/97W97s54OFRYWKhgMRtrZs2f7/DkBADen2O5stGrVKu3cuVMHDhzQyJEjI8t9Pp9aWlpUV1cXddZVVVUln88X6fP+++9H7e/KrMMrfb7O4/HI4/F0p1QAwADTpTMuY4xWrVqlbdu2ae/evRo9enTU+qlTpyouLk579uyJLCstLVVFRYX8fr8kye/365NPPlF1dXWkz+7du+X1ejVp0qSeHAsAYDDoykyOlStXmsTERLN//35TWVkZaY2NjZE+K1asMFlZWWbv3r3m6NGjxu/3G7/fH1nf1tZmsrOzzezZs83x48fNO++8Y0aMGGEKCws7XQezCmk0Gm1gtO7MKuxScF3riTdt2hTp09TUZJ544glz2223mYSEBPPd737XVFZWRu3niy++MHPmzDFDhgwxKSkpZs2aNaa1tbXTdRBcNBqNNjBad4LL9b+BZJVQKKTExESnywAA9FAwGJTX6+3SNnxXIQDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKl0KrqKiIk2fPl3Dhg1Tamqq5s+fr9LS0qg+DzzwgFwuV1RbsWJFVJ+KigrNnTtXCQkJSk1N1dq1a9XW1tbzowEADHixXelcXFysgoICTZ8+XW1tbXr22Wc1e/ZsnTp1Srfeemuk37Jly/TCCy9EHickJET+bm9v19y5c+Xz+XTo0CFVVlbqBz/4geLi4vQ3f/M3vXBIAIABzfRAdXW1kWSKi4sjy+6//37z4x//+JrbvP3228btdptAIBBZ9vLLLxuv12uam5s79bzBYNBIotFoNJrlLRgMdjl7enSPKxgMSpKSk5Ojlv/mN79RSkqKsrOzVVhYqMbGxsi6kpISTZ48WWlpaZFleXl5CoVCOnnyZIfP09zcrFAoFNUAAINTly4V/l/hcFhPPvmk7rvvPmVnZ0eWP/bYYxo1apQyMjL08ccf65lnnlFpaanefPNNSVIgEIgKLUmRx4FAoMPnKioq0oYNG7pbKgBgAOl2cBUUFOjEiRM6ePBg1PLly5dH/p48ebLS09M1a9YsnTlzRmPHju3WcxUWFmr16tWRx6FQSJmZmd0rHABgtW5dKly1apV27typffv2aeTIkdftm5OTI0kqKyuTJPl8PlVVVUX1ufLY5/N1uA+PxyOv1xvVAACDU5eCyxijVatWadu2bdq7d69Gjx59w22OHz8uSUpPT5ck+f1+ffLJJ6quro702b17t7xeryZNmtSVcgAAg1FXZnKsXLnSJCYmmv3795vKyspIa2xsNMYYU1ZWZl544QVz9OhRU15ebt566y0zZswYM3PmzMg+2traTHZ2tpk9e7Y5fvy4eeedd8yIESNMYWFhp+tgViGNRqMNjNadWYVdCq5rPfGmTZuMMcZUVFSYmTNnmuTkZOPxeMwdd9xh1q5de1VhX3zxhZkzZ44ZMmSISUlJMWvWrDGtra2droPgotFotIHRuhNcrv8NJKuEQiElJiY6XQYAoIeCwWCX5y1Y+V2FFmYtAKAD3fn33MrgunTpktMlAAB6QXf+PbfyUmE4HFZpaakmTZqks2fPMj2+A1c+68b4dIzxuT7G58YYo+u70fgYY3Tp0iVlZGTI7e7aOVS3P4DsJLfbrW984xuSxOe6boDxuT7G5/oYnxtjjK7veuPT3bkKVl4qBAAMXgQXAMAq1gaXx+PR+vXr5fF4nC7lpsT4XB/jc32Mz40xRtfXl+Nj5eQMAMDgZe0ZFwBgcCK4AABWIbgAAFYhuAAAVrEyuDZu3Kjbb79dt9xyi3JycvT+++87XZIjnn/+eblcrqg2YcKEyPrLly+roKBAw4cP19ChQ7Vw4cKrfsRzoDlw4IDmzZunjIwMuVwubd++PWq9MUbr1q1Tenq6hgwZotzcXJ0+fTqqT21trRYvXiyv16ukpCQtXbpU9fX1/XgUfedG4/PDH/7wqtdUfn5+VJ+BOj5FRUWaPn26hg0bptTUVM2fP1+lpaVRfTrznqqoqNDcuXOVkJCg1NRUrV27Vm1tbf15KH2mM2P0wAMPXPUaWrFiRVSfno6RdcH1+uuva/Xq1Vq/fr3+8Ic/aMqUKcrLy4v6YcrB5K677lJlZWWkHTx4MLLuqaee0o4dO7R161YVFxfr/PnzWrBggYPV9r2GhgZNmTJFGzdu7HD9iy++qF/+8pd65ZVXdOTIEd16663Ky8vT5cuXI30WL16skydPavfu3dq5c6cOHDig5cuX99ch9KkbjY8k5efnR72mtmzZErV+oI5PcXGxCgoKdPjwYe3evVutra2aPXu2GhoaIn1u9J5qb2/X3Llz1dLSokOHDunVV1/V5s2btW7dOicOqdd1ZowkadmyZVGvoRdffDGyrlfGqMs/hOKwGTNmmIKCgsjj9vZ2k5GRYYqKihysyhnr1683U6ZM6XBdXV2diYuLM1u3bo0s+/TTT40kU1JS0k8VOkuS2bZtW+RxOBw2Pp/PvPTSS5FldXV1xuPxmC1bthhjjDl16pSRZD744INIn127dhmXy2W+/PLLfqu9P3x9fIwxZsmSJebhhx++5jaDaXyqq6uNJFNcXGyM6dx76u233zZut9sEAoFIn5dfftl4vV7T3NzcvwfQD74+RsYYc//995sf//jH19ymN8bIqjOulpYWHTt2TLm5uZFlbrdbubm5KikpcbAy55w+fVoZGRkaM2aMFi9erIqKCknSsWPH1NraGjVWEyZMUFZW1qAdq/LycgUCgagxSUxMVE5OTmRMSkpKlJSUpGnTpkX65Obmyu1268iRI/1esxP279+v1NRUjR8/XitXrlRNTU1k3WAan2AwKElKTk6W1Ln3VElJiSZPnqy0tLRIn7y8PIVCIZ08ebIfq+8fXx+jK37zm98oJSVF2dnZKiwsVGNjY2Rdb4yRVV+ye/HiRbW3t0cdsCSlpaXps88+c6gq5+Tk5Gjz5s0aP368KisrtWHDBn3729/WiRMnFAgEFB8fr6SkpKht0tLSFAgEnCnYYVeOu6PXz5V1gUBAqampUetjY2OVnJw8KMYtPz9fCxYs0OjRo3XmzBk9++yzmjNnjkpKShQTEzNoxiccDuvJJ5/Ufffdp+zsbEnq1HsqEAh0+Pq6sm4g6WiMJOmxxx7TqFGjlJGRoY8//ljPPPOMSktL9eabb0rqnTGyKrgQbc6cOZG/7777buXk5GjUqFF64403NGTIEAcrg60effTRyN+TJ0/W3XffrbFjx2r//v2aNWuWg5X1r4KCAp04cSLqnjGiXWuM/u/9zsmTJys9PV2zZs3SmTNnNHbs2F55bqsuFaakpCgmJuaqWTxVVVXy+XwOVXXzSEpK0p133qmysjL5fD61tLSorq4uqs9gHqsrx32914/P57tqok9bW5tqa2sH5biNGTNGKSkpKisrkzQ4xmfVqlXauXOn9u3bp5EjR0aWd+Y95fP5Onx9XVk3UFxrjDqSk5MjSVGvoZ6OkVXBFR8fr6lTp2rPnj2RZeFwWHv27JHf73ewsptDfX29zpw5o/T0dE2dOlVxcXFRY1VaWqqKiopBO1ajR4+Wz+eLGpNQKKQjR45ExsTv96uurk7Hjh2L9Nm7d6/C4XDkDTiYnDt3TjU1NUpPT5c0sMfHGKNVq1Zp27Zt2rt3r0aPHh21vjPvKb/fr08++SQq3Hfv3i2v16tJkyb1z4H0oRuNUUeOHz8uSVGvoR6PUTcnkzjmt7/9rfF4PGbz5s3m1KlTZvny5SYpKSlqhspgsWbNGrN//35TXl5u3nvvPZObm2tSUlJMdXW1McaYFStWmKysLLN3715z9OhR4/f7jd/vd7jqvnXp0iXz4Ycfmg8//NBIMr/4xS/Mhx9+aP74xz8aY4z527/9W5OUlGTeeust8/HHH5uHH37YjB492jQ1NUX2kZ+fb/7sz/7MHDlyxBw8eNCMGzfOLFq0yKlD6lXXG59Lly6Zp59+2pSUlJjy8nLz+9//3txzzz1m3Lhx5vLly5F9DNTxWblypUlMTDT79+83lZWVkdbY2Bjpc6P3VFtbm8nOzjazZ882x48fN++8844ZMWKEKSwsdOKQet2NxqisrMy88MIL5ujRo6a8vNy89dZbZsyYMWbmzJmRffTGGFkXXMYY84//+I8mKyvLxMfHmxkzZpjDhw87XZIjHnnkEZOenm7i4+PNN77xDfPII4+YsrKyyPqmpibzxBNPmNtuu80kJCSY7373u6aystLBivvevn37jKSr2pIlS4wxf5oS/9xzz5m0tDTj8XjMrFmzTGlpadQ+ampqzKJFi8zQoUON1+s1jz/+uLl06ZIDR9P7rjc+jY2NZvbs2WbEiBEmLi7OjBo1yixbtuyq/ykcqOPT0bhIMps2bYr06cx76osvvjBz5swxQ4YMMSkpKWbNmjWmtbW1n4+mb9xojCoqKszMmTNNcnKy8Xg85o477jBr1641wWAwaj89HSN+1gQAYBWr7nEBAEBwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKzy/wDTG77gx8h10AAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_image = cv2.imread(r\"F:\\FYP\\FYP\\FigshareDataset\\training\\masks\\2.png\",0)\n",
        "test_image = cv2.resize(test_image,(256,256))\n",
        "test_image = (test_image/255).astype(int).astype(float)\n",
        "print(np.unique(test_image,return_counts=True))\n",
        "plt.imshow(test_image, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GaSfIG6WmeI"
      },
      "source": [
        "# Creating the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkfksE-dXGP9"
      },
      "outputs": [],
      "source": [
        "# def load_img(img_dir, img_list):\n",
        "#     images=[]\n",
        "#     for i, image_name in enumerate(img_list):\n",
        "#         if (image_name.split('.')[1] == 'npy'):\n",
        "#             image = np.load(img_dir+image_name)\n",
        "#             # print(\"Image - \",image.shape)\n",
        "#             images.append(image)\n",
        "#     images = np.array(images)\n",
        "\n",
        "#     return(images)\n",
        "\n",
        "\n",
        "\n",
        "def load_img(img_dir, img_list):\n",
        "    images=[]\n",
        "    for i, image_name in enumerate(img_list):\n",
        "        if (image_name.split('.')[1] == 'png'):\n",
        "            image = cv2.imread(img_dir+\"/\"+image_name,0)\n",
        "            image = cv2.resize(image,(256,256))\n",
        "            images.append(image)\n",
        "    images = np.array(images)\n",
        "\n",
        "    return(images)\n",
        "\n",
        "\n",
        "def load_msk(img_dir, img_list):\n",
        "    images=[]\n",
        "    for i, image_name in enumerate(img_list):\n",
        "        if (image_name.split('.')[1] == 'png'):\n",
        "            image = (cv2.imread(img_dir+\"/\"+image_name,0)/255).astype(float)\n",
        "            # print(np.unique(image))\n",
        "            image = cv2.resize(image,(256,256))\n",
        "            # print(\"before - \",image.shape)\n",
        "            image = to_categorical(image, num_classes=2)\n",
        "            # print(\"After - \",image.shape)\n",
        "            images.append(image)\n",
        "    images = np.array(images)\n",
        "\n",
        "    return(images)\n",
        "\n",
        "def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size):\n",
        "\n",
        "    L = len(img_list)\n",
        "\n",
        "    #keras needs the generator infinite, so we will use while true\n",
        "    while True:\n",
        "\n",
        "        batch_start = 0\n",
        "        batch_end = batch_size\n",
        "\n",
        "        while batch_start < L:\n",
        "            limit = min(batch_end, L)\n",
        "\n",
        "            X = load_img(img_dir, img_list[batch_start:limit])\n",
        "            Y = load_msk(mask_dir, mask_list[batch_start:limit])\n",
        "\n",
        "            yield (X,Y) #a tuple with two numpy arrays with batch_size samples\n",
        "            batch_start += batch_size\n",
        "            batch_end += batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo5ycNUKXM1g"
      },
      "source": [
        "# Making the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9u3OW40YIjy"
      },
      "outputs": [],
      "source": [
        "train_img_dir = r\"F:\\FYP\\FYP\\FigshareDataset\\images\"\n",
        "train_mask_dir = r\"F:\\FYP\\FYP\\FigshareDataset\\masks\"\n",
        "train_img_list=os.listdir(train_img_dir)\n",
        "train_mask_list = os.listdir(train_mask_dir)\n",
        "\n",
        "# val_img_dir = \"/content/drive/MyDrive/BRATS_train_data/X_validation/\"\n",
        "# val_mask_dir = \"/content/drive/MyDrive/BRATS_train_data/Y_validation/\"\n",
        "# val_img_list=os.listdir(val_img_dir)\n",
        "# val_mask_list = os.listdir(val_mask_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqqnGK6PXMCe"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_img_datagen = imageLoader(train_img_dir, train_img_list,\n",
        "                                train_mask_dir, train_mask_list, batch_size)\n",
        "\n",
        "# val_img_datagen = imageLoader(val_img_dir, val_img_list,\n",
        "#                                 val_mask_dir, val_mask_list, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO3f9BZXXfGf"
      },
      "source": [
        "# Verifying the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdPUPs1fXc4r"
      },
      "outputs": [],
      "source": [
        "#Verify generator.... In python 3 next() is renamed as __next__()\n",
        "img, msk = train_img_datagen.__next__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoY-LQEQZB3U"
      },
      "source": [
        "# Visualizing the loaded images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "dODfFiTVbJ7j",
        "outputId": "b759fdd0-2342-4dab-8b11-745e0b83af94"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "data_point = random.randint(0, batch_size)\n",
        "# data_point = 3\n",
        "\n",
        "original_labels = np.expand_dims(msk[data_point,:,:,:], axis=0)\n",
        "original_labels = np.argmax(original_labels, axis=3)\n",
        "print(original_labels.shape)\n",
        "\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(img[data_point,:], cmap='gray')\n",
        "plt.title('Scan')\n",
        "plt.subplot(122)\n",
        "plt.imshow(original_labels[0,:,:], cmap='gray')\n",
        "plt.title('Complete label')\n",
        "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0tjF0qzhdsX"
      },
      "source": [
        "# Defining 3 UNET architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQwDicNzhick"
      },
      "outputs": [],
      "source": [
        "# https://youtu.be/L5iV5BHkMzM\n",
        "\"\"\"\n",
        "\n",
        "Attention U-net:\n",
        "https://arxiv.org/pdf/1804.03999.pdf\n",
        "\n",
        "Recurrent residual Unet (R2U-Net) paper\n",
        "https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n",
        "(Check fig 4.)\n",
        "\n",
        "Note: Batch normalization should be performed over channels after a convolution,\n",
        "In the following code axis is set to 3 as our inputs are of shape\n",
        "[None, height, width, channel]. Channel is axis=3.\n",
        "\n",
        "Original code from below link but heavily modified.\n",
        "https://github.com/MoleImg/Attention_UNet/blob/master/AttResUNet.py\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "A few useful metrics and losses\n",
        "'''\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "##############################################################\n",
        "'''\n",
        "Useful blocks to build Unet\n",
        "\n",
        "conv - BN - Activation - conv - BN - Activation - Dropout (if enabled)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
        "\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\",kernel_initializer='he_normal',activation = \"relu\")(x)\n",
        "    conv = layers.Dropout(dropout)(conv)\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\",kernel_initializer='he_normal',activation = \"relu\")(conv)\n",
        "    return conv\n",
        "\n",
        "\n",
        "def repeat_elem(tensor, rep):\n",
        "    # lambda function to repeat Repeats the elements of a tensor along an axis\n",
        "    #by a factor of rep.\n",
        "    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape\n",
        "    #(None, 256,256,6), if specified axis=3 and rep=2.\n",
        "\n",
        "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
        "                          arguments={'repnum': rep})(tensor)\n",
        "\n",
        "\n",
        "def res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
        "    '''\n",
        "    Residual convolutional layer.\n",
        "    Two variants....\n",
        "    Either put activation function before the addition with shortcut\n",
        "    or after the addition (which would be as proposed in the original resNet).\n",
        "\n",
        "    1. conv - BN - Activation - conv - BN - Activation\n",
        "                                          - shortcut  - BN - shortcut+BN\n",
        "\n",
        "    2. conv - BN - Activation - conv - BN\n",
        "                                     - shortcut  - BN - shortcut+BN - Activation\n",
        "\n",
        "    Check fig 4 in https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n",
        "    '''\n",
        "\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)\n",
        "    if batch_norm is True:\n",
        "        conv = layers.BatchNormalization(axis=3)(conv)\n",
        "    conv = layers.Activation('relu')(conv)\n",
        "\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n",
        "    if batch_norm is True:\n",
        "        conv = layers.BatchNormalization(axis=3)(conv)\n",
        "    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n",
        "    if dropout > 0:\n",
        "        conv = layers.Dropout(dropout)(conv)\n",
        "\n",
        "    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n",
        "    if batch_norm is True:\n",
        "        shortcut = layers.BatchNormalization(axis=3)(shortcut)\n",
        "\n",
        "    res_path = layers.add([shortcut, conv])\n",
        "    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)\n",
        "    return res_path\n",
        "\n",
        "def gating_signal(input, out_size, batch_norm=False):\n",
        "    \"\"\"\n",
        "    resize the down layer feature map into the same dimension as the up layer feature map\n",
        "    using 1x1 conv\n",
        "    :return: the gating feature map with the same dimension of the up layer feature map\n",
        "    \"\"\"\n",
        "    x = layers.Conv2D(out_size, (1, 1), padding='same',kernel_initializer='he_normal')(input)\n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def attention_block(x, gating, inter_shape):\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gating)\n",
        "\n",
        "# Getting the x signal to the same shape as the gating signal\n",
        "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same',kernel_initializer='he_normal')(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "# Getting the gating signal to the same number of filters as the inter_shape\n",
        "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same',kernel_initializer='he_normal')(gating)\n",
        "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
        "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
        "                                 padding='same')(phi_g)  # 16\n",
        "\n",
        "    concat_xg = layers.add([upsample_g, theta_x])\n",
        "    act_xg = layers.Activation('relu')(concat_xg)\n",
        "    psi = layers.Conv2D(1, (1, 1), padding='same',kernel_initializer='he_normal')(act_xg)\n",
        "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
        "\n",
        "    y = layers.multiply([upsample_psi, x])\n",
        "\n",
        "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same',kernel_initializer='he_normal')(y)\n",
        "    result_bn = layers.BatchNormalization()(result)\n",
        "    return result_bn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def UNet(input_shape, NUM_CLASSES=2, dropout_rate=0.4, batch_norm=True):\n",
        "    '''\n",
        "    UNet,\n",
        "\n",
        "    '''\n",
        "    # network structure\n",
        "    FILTER_NUM = 16 # number of filters for the first layer\n",
        "    FILTER_SIZE = 3 # size of the convolutional filter\n",
        "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
        "\n",
        "\n",
        "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
        "\n",
        "    # Downsampling layers\n",
        "    # DownRes 1, convolution + pooling\n",
        "    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, 0.3, batch_norm)#Conv2D > dropout > Conv2D\n",
        "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
        "    # DownRes 2\n",
        "    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, 0.3, batch_norm)\n",
        "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
        "    # DownRes 3\n",
        "    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, 0.4, batch_norm)\n",
        "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
        "    # DownRes 4\n",
        "    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, 0.4, batch_norm)\n",
        "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
        "    # DownRes 5, convolution only\n",
        "    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, 0.5, batch_norm)\n",
        "\n",
        "    # Upsampling layers\n",
        "\n",
        "    #up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
        "    up_16 = layers.Conv2DTranspose(8*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(conv_8)\n",
        "    up_16 = layers.concatenate([up_16, conv_16], axis=3)\n",
        "    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, 0.4, batch_norm)\n",
        "    # UpRes 7\n",
        "\n",
        "    up_32 = layers.Conv2DTranspose(4*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_16)\n",
        "    up_32 = layers.concatenate([up_32, conv_32], axis=3)\n",
        "    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, 0.4, batch_norm)\n",
        "    # UpRes 8\n",
        "\n",
        "    up_64 = layers.Conv2DTranspose(2*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_32)\n",
        "    up_64 = layers.concatenate([up_64, conv_64], axis=3)\n",
        "    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, 0.3, batch_norm)\n",
        "    # UpRes 9\n",
        "\n",
        "    up_128 = layers.Conv2DTranspose(FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_64)\n",
        "    up_128 = layers.concatenate([up_128, conv_128], axis=3)\n",
        "    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, 0.3, batch_norm)\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "\n",
        "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1),kernel_initializer='he_normal',activation = 'sigmoid')(up_conv_128)\n",
        "    #conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    #conv_final = layers.Activation('softmax')(conv_final)  #Change to softmax for multichannel\n",
        "\n",
        "    # Model\n",
        "    model = models.Model(inputs, conv_final, name=\"UNet\")\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def Attention_UNet(input_shape, NUM_CLASSES=4, dropout_rate=0.4, batch_norm=False):\n",
        "    '''\n",
        "    Attention UNet,\n",
        "\n",
        "    '''\n",
        "    # network structure\n",
        "    FILTER_NUM = 16 # number of basic filters for the first layer\n",
        "    FILTER_SIZE = 3 # size of the convolutional filter\n",
        "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
        "\n",
        "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
        "\n",
        "    # Downsampling layers\n",
        "    # DownRes 1, convolution + pooling\n",
        "    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, 0.3, batch_norm)#Conv2D > dropout > Conv2D\n",
        "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
        "    # DownRes 2\n",
        "    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, 0.3, batch_norm)\n",
        "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
        "    # DownRes 3\n",
        "    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, 0.4, batch_norm)\n",
        "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
        "    # DownRes 4\n",
        "    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, 0.4, batch_norm)\n",
        "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
        "    # DownRes 5, convolution only\n",
        "    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, 0.5, batch_norm)\n",
        "\n",
        "    # Upsampling layers\n",
        "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
        "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
        "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
        "    up_16 = layers.Conv2DTranspose(8*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(conv_8)\n",
        "    up_16 = layers.concatenate([up_16, att_16], axis=3)\n",
        "    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, 0.4, batch_norm)\n",
        "    # UpRes 7\n",
        "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
        "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
        "    up_32 = layers.Conv2DTranspose(4*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_16)\n",
        "    up_32 = layers.concatenate([up_32, att_32], axis=3)\n",
        "    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, 0.4, batch_norm)\n",
        "    # UpRes 8\n",
        "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
        "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
        "    up_64 = layers.Conv2DTranspose(2*FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_32)\n",
        "    up_64 = layers.concatenate([up_64, att_64], axis=3)\n",
        "    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, 0.3, batch_norm)\n",
        "    # UpRes 9\n",
        "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
        "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
        "    up_128 = layers.Conv2DTranspose(FILTER_NUM, (2, 2), strides=(2, 2), padding='same')(up_conv_64)\n",
        "    up_128 = layers.concatenate([up_128, att_128], axis=3)\n",
        "    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, 0.3, batch_norm)\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1),kernel_initializer='he_normal',activation = 'softmax')(up_conv_128)\n",
        "    #conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    #conv_final = layers.Activation('softmax')(conv_final)  #Change to softmax for multichannel\n",
        "\n",
        "    # Model integration\n",
        "    model = models.Model(inputs, conv_final, name=\"Attention_UNet\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def Attention_ResUNet(input_shape, NUM_CLASSES=4, dropout_rate=0.4, batch_norm=True):\n",
        "    '''\n",
        "    Rsidual UNet, with attention\n",
        "\n",
        "    '''\n",
        "    # network structure\n",
        "    FILTER_NUM = 32 # number of basic filters for the first layer\n",
        "    FILTER_SIZE = 3 # size of the convolutional filter\n",
        "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
        "    # input data\n",
        "    # dimension of the image depth\n",
        "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
        "    axis = 3\n",
        "\n",
        "    # Downsampling layers\n",
        "    # DownRes 1, double residual convolution + pooling\n",
        "    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
        "    # DownRes 2\n",
        "    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
        "    # DownRes 3\n",
        "    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
        "    # DownRes 4\n",
        "    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
        "    # DownRes 5, convolution only\n",
        "    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # Upsampling layers\n",
        "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
        "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
        "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
        "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
        "    up_16 = layers.concatenate([up_16, att_16], axis=axis)\n",
        "    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 7\n",
        "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
        "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
        "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
        "    up_32 = layers.concatenate([up_32, att_32], axis=axis)\n",
        "    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 8\n",
        "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
        "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
        "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
        "    up_64 = layers.concatenate([up_64, att_64], axis=axis)\n",
        "    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 9\n",
        "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
        "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
        "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
        "    up_128 = layers.concatenate([up_128, att_128], axis=axis)\n",
        "    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "\n",
        "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n",
        "    conv_final = layers.BatchNormalization(axis=axis)(conv_final)\n",
        "    conv_final = layers.Activation('softmax')(conv_final)  #Change to softmax for multichannel\n",
        "\n",
        "    # Model integration\n",
        "    model = models.Model(inputs, conv_final, name=\"AttentionResUNet\")\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en-rIizgiZZY"
      },
      "outputs": [],
      "source": [
        "# input_shape = (240,240,4)\n",
        "# # UNet(input_shape, NUM_CLASSES=4, dropout_rate=0.0, batch_norm=True)\n",
        "# # Attention_UNet(input_shape, NUM_CLASSES=4, dropout_rate=0.0, batch_norm=True)\n",
        "# Attention_ResUNet(input_shape, NUM_CLASSES=4, dropout_rate=0.0, batch_norm=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH-O4iCbjhcU"
      },
      "outputs": [],
      "source": [
        "###############################################################\n",
        "# from sklearn.utils import class_weight\n",
        "# class_weights = class_weight.compute_class_weight('balanced',\n",
        "#                                                  np.unique(train_masks_reshaped_encoded),\n",
        "#                                                  train_masks_reshaped_encoded)\n",
        "# print(\"Class weights are...:\", class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7zpyoBZT3CI"
      },
      "source": [
        "# Defining loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNG05jfhT2BZ"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)  # -1 ultiplied as we want to minimize this value as loss function\n",
        "\n",
        "# # Typical tf.keras API usage\n",
        "# import tensorflow as tf\n",
        "# from focal_loss import BinaryFocalLoss,SparseCategoricalFocalLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94qTpaQoUDgS"
      },
      "outputs": [],
      "source": [
        "# import segmentation_models_3D as sm\n",
        "# dice_loss = sm.losses.DiceLoss(class_weights=np.array([wt0, wt1, wt2, wt3]))\n",
        "# focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "# total_loss = dice_loss + (1 * focal_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4Y1Ji2hVWl5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self, alpha=0.25, gamma=4.0):\n",
        "    super(FocalLoss, self).__init__()\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def call(self, y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Inputs and targets should be of shape (batch_size, num_classes, height, width)\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the cross-entropy loss\n",
        "    ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Calculate the scaling factor\n",
        "    pt = tf.math.exp(-ce_loss)\n",
        "    #scaling_factor = self.alpha * (1 - pt) ** self.gamma\n",
        "    scaling_factor = (1 - pt) ** self.gamma\n",
        "\n",
        "    # Calculate the Focal loss\n",
        "    focal_loss = scaling_factor * ce_loss\n",
        "\n",
        "    # Return the average Focal loss over the batch\n",
        "    return tf.reduce_mean(focal_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqpXKkoHNeNY"
      },
      "source": [
        "# Creating a callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEA_6NadLL2A"
      },
      "outputs": [],
      "source": [
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"/content/drive/MyDrive/models/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Calculate the number of batches per epoch\n",
        "import math\n",
        "n_batches = 3000 / batch_size\n",
        "n_batches = math.ceil(n_batches)    # round up the number of batches to the nearest whole integer\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=10*n_batches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Drrib_NjEU0",
        "outputId": "499a3292-259c-41a0-a4c6-1e174d83e970"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH  =  256\n",
        "IMG_CHANNELS = 1\n",
        "\n",
        "\n",
        "input_shape = (IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS)\n",
        "\n",
        "\n",
        "model = UNet(input_shape, NUM_CLASSES=2, dropout_rate=0.4, batch_norm=False)\n",
        "# model.compile(optimizer='adam', loss= tf.keras.losses.CategoricalFocalCrossentropy(alpha=1,gamma=4), metrics=[jacard_coef])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtMbEkG3qVa0"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = len(train_img_list)//batch_size\n",
        "# val_steps_per_epoch = len(val_img_list)//batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "YywvHDtpjG9z",
        "outputId": "4425699d-9bf8-4e93-ec00-cfe6631d6618"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_img_datagen,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    verbose=1,\n",
        "                    epochs=103,\n",
        "                    # validation_data=val_img_datagen,\n",
        "                    # validation_steps=val_steps_per_epoch,\n",
        "                    shuffle=False,\n",
        "                    # callbacks=[cp_callback],\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuoWi4lTkPc6"
      },
      "outputs": [],
      "source": [
        "###\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGELS4ok1g2x"
      },
      "outputs": [],
      "source": [
        "for i in history.history.items():\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOOR87_9kYxy"
      },
      "outputs": [],
      "source": [
        "acc = history.history['jacard_coef']\n",
        "val_acc = history.history['val_jacard_coef']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcBdFfESDgF3"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/drive/MyDrive/models/training_loss.npy\",np.array(loss))\n",
        "np.save(\"/content/drive/MyDrive/models/val_loss.npy\",np.array(val_loss))\n",
        "np.save(\"/content/drive/MyDrive/models/training_jaccard.npy\",np.array(acc))\n",
        "np.save(\"/content/drive/MyDrive/models/val_jaccard.npy\",np.array(val_acc))\n",
        "np.save(\"/content/drive/MyDrive/models/epochs.npy\",range(1, len(np.array(loss)) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJDNrwwN1J_"
      },
      "outputs": [],
      "source": [
        "os.listdir(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYkv2D_5N47t"
      },
      "outputs": [],
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUblHKOOOD2e"
      },
      "outputs": [],
      "source": [
        "# Create a new model instance\n",
        "\n",
        "model = Attention_UNet(input_shape, NUM_CLASSES=4, dropout_rate=0.2, batch_norm=True)\n",
        "model.compile(optimizer='adam', loss= FocalLoss(), metrics=[jacard_coef])\n",
        "\n",
        "# Load the previously saved weights\n",
        "model.load_weights(\"/content/drive/MyDrive/models/cp-0041.ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFOGEiJDGXdx"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_img_datagen,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    verbose=1,\n",
        "                    epochs=100,\n",
        "                    validation_data=val_img_datagen,\n",
        "                    validation_steps=val_steps_per_epoch,\n",
        "                    shuffle=False,\n",
        "                    callbacks=[cp_callback],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6w7tqWZke9a"
      },
      "outputs": [],
      "source": [
        "#Predict on a few images\n",
        "#model = get_model()\n",
        "#model.load_weights('???.hdf5')\n",
        "img, msk = val_img_datagen.__next__()\n",
        "import random\n",
        "test_img_number = random.randint(0, len(img))#6 is good\n",
        "test_img = img[test_img_number]\n",
        "ground_truth=msk[test_img_number]\n",
        "# test_img_norm=test_img[:,:,0][:,:,None]\n",
        "test_img_input=np.expand_dims(test_img, 0)\n",
        "prediction = (model.predict(test_img_input))\n",
        "predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
        "print(prediction.shape)\n",
        "print(ground_truth.shape)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img[:,:,0], cmap='gray') #only the first channel is displayed\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.title('Original Label')\n",
        "displayed_label = np.argmax(ground_truth, axis=2)\n",
        "plt.imshow(displayed_label, cmap='jet')\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.title('Predicted Label')\n",
        "plt.imshow(predicted_img, cmap='jet')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsYwxbvVjlJp"
      },
      "outputs": [],
      "source": [
        "print(test_img_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK21FSbuoPNx"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/models/my_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qth9Izdk385O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
